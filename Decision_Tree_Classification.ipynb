{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9oteschgjaS+gs3ZUGgd0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girupashankar/Machine_Learning/blob/main/Decision_Tree_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Classification Explained\n",
        "\n",
        "Decision Tree is a popular and widely used machine learning algorithm for classification and regression tasks. It works by recursively splitting the dataset into subsets based on the most significant feature at each node. This process creates a tree-like structure where the leaves represent the class labels or regression values.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Nodes**: Represent features or attributes in the dataset.\n",
        "2. **Edges**: Represent the decision rules based on feature values.\n",
        "3. **Root Node**: The topmost node that corresponds to the best predictor.\n",
        "4. **Internal Nodes**: Represent features and are used for decision making.\n",
        "5. **Leaf Nodes**: Represent the class labels or regression values.\n",
        "6. **Decision Rules**: Determined by the feature values at each node.\n",
        "\n",
        "### How Decision Tree Classification Works\n",
        "\n",
        "1. **Select the Best Split**: Determine the best feature to split the dataset. Common metrics include Gini impurity and information gain.\n",
        "2. **Split the Dataset**: Split the dataset into subsets based on the selected feature.\n",
        "3. **Repeat**: Recursively apply the above steps to each subset until all data points in a subset belong to the same class or a subset contains a specified number of data points.\n",
        "4. **Create the Tree**: The result is a tree with decision nodes and leaf nodes."
      ],
      "metadata": {
        "id": "M3qizOmUpGUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Iris Dataset Classification\n",
        "\n",
        "Let's use the Iris dataset to classify iris flowers into three different species based on features like sepal length, sepal width, petal length, and petal width."
      ],
      "metadata": {
        "id": "eBLbRHH9pLaJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKAnMNoVoZl6",
        "outputId": "f61d2f88-e3e2-446f-ba12-1739118a099b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the Decision Tree classifier model\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Model evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Code\n",
        "\n",
        "1. **Data Preparation**: We load the Iris dataset and split it into features (X) and target variable (y).\n",
        "2. **Train-Test Split**: We split the data into training and testing sets for model evaluation.\n",
        "3. **Model Creation and Training**: We create a DecisionTreeClassifier instance and fit it to the training data.\n",
        "4. **Prediction and Evaluation**: We predict iris species on the test set and evaluate the model using accuracy, confusion matrix, and classification report.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Decision Tree is a versatile algorithm that is easy to interpret and understand. It can handle both numerical and categorical data and is robust against outliers. However, decision trees can be prone to overfitting, especially with deep trees and complex datasets. Techniques like pruning, setting a maximum depth, or using ensemble methods like Random Forest can help mitigate overfitting.\n",
        "\n",
        "If you have any specific questions or need further details on any part of this explanation, feel free to ask! ðŸ˜Š"
      ],
      "metadata": {
        "id": "bhk3Jyw4o_7t"
      }
    }
  ]
}